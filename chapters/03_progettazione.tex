\chapter{Progettazione}

In questo capitolo viene approfondita l'analisi del problema delineato nel Capitolo~\ref{chap:contesto}, con particolare riferimento all'applicazione di tecniche di Named Entity Recognition per l'estrazione automatica di informazioni da documenti tecnici in formato PDF.

I documenti oggetto di studio sono forniti da enti esterni e risultano progettati principalmente per la consultazione umana. Essi non seguono standard strutturali che favoriscono l'elaborazione automatica, anche a causa della presenza di molteplici soggetti a produrli, ciascuno dei quali adotta layout e impaginazioni differenti. La complessità della struttura è accentuata dall'utilizzo di tabelle, colonne multiple e testi multilingua, elementi che impediscono l'estrazione sistematica delle informazioni.

[immagine esempio documento]

Il problema affrontato nel progetto consiste nell'individuare una strategia efficace per trasformare documenti eterogenei e rumorosi in una rappresentazione strutturata, attraverso l'identificazione automatica di entità rilevanti. A tal fine sono stati presi in considerazione due approcci differenti: una soluzione cloud-based basata su un prodotto già disponibile sul mercato e una soluzione custom che implementa l'intero processo, dalla fase di estrazione del contenuto testuale del file fino all'addestramento e all'utilizzo del modello dedicato per l'estrazione automatica delle entità.

\section{Caso d'uso}

I casi d'uso descrivono l'interazione tra l'utente e l'applicazione. Essi sono essenziali per rappresentare le principali interazioni, definire i requisiti funzionali e non funzionali e orientare lo sviluppo dell'applicazione.

La definizione dei casi d'uso è stata condotta a partire dalle esigenze emerse nelle fasi di analisi precedenti e formalizzate attraverso il confronto con un Business Analyst, che ha contribuito a delineare il processo richiesto. In primo luogo sono stati definiti gli attori coinvolti nel sistema:

\begin{itemize}
    \item \textbf{Interfaccia Web}: la componente che gestisce il flusso applicativo con cui interagiscono gli utenti;
    \item \textbf{Utente}: colui che utilizza l'applicazione web per caricare i documenti di report; in questo contesto si tratta di un dipendente di un ente esterno che si occupa di certificazioni e test di laboratorio;
    \item \textbf{Servizio NER}: il servizio applicativo che utilizza il modello addestrato per estrarre le informazioni significative dal documento;
    \item \textbf{Database}: il contenitore dei dati relativi ai modelli (prodotti finiti), agli articoli e alle informazioni sui test necessari per il processo produttivo.
\end{itemize}

[diagramma flusso]

Successivamente, è stato definito nel dettaglio il flusso applicativo desiderato, che ha lo scopo di semplificare il processo di aggiornamento dei test effettuati su un determinato articolo o modello. Di seguito vengono definite le fasi che caratterizzano tale flusso.

\textbf{Caricamento del report}.   L'utente dell'ente di certificazione accede a una pagina dedicata all'interno dell'applicativo, affiancata a quelle già esistenti e distinta da esse per la semplicità dell'interfaccia. All'apertura della pagina infatti, l'utente visualizza solo vedere un modulo di caricamento che consente di effettuare l'upload del documento PDF, o tramite trascinamento o tramite ricerca nelle cartelle del dispositivo utilizzato. Il documento racchiude i risultati dei test di laboratorio eseguiti su uno specifico articolo o modello.

\textbf{Elaborazione del report}.    Il file viene inoltrato a un servizio di elaborazione che si occupa di caricare il modello NER, che in questo caso potrà corrispondere o al modello cloud-based o a quello implementato custom, e lo utilizzerà per analizzare il file e restituire le entità significative per cui il modello è addestrato. Il formato per poter essere utilizzabile dall'applicazione dev'essere standardizzato per entrambe le soluzioni realizzate.

\textbf{Ricerca delle similarità}.   I dati ottenuti dal modello NER devono essere utilizzati per individuare all'interno del database i record che hanno una corrispondenza con i dati ottenuti dal report, ovvero le singole prove dell'articolo o modello. I \textit{match} individuati non devono essere necessariamente esatti, ma devono contenere i risultati più plausibili in base ai dati a disposizione, per poi essere mostrati all'utente tramite interfaccia.

\textbf{Approvazione dei risultati}. Con la lista dei record a disposizione, l'utente può validare i test effettivamente svolti, oppure in caso di non corrispondenza con ciò che è presente nel report, ha la possibilità di passare alla ricerca manuale sempre tramite interfaccia, utilizzando filtri precompilati a partire dai risultati ottenuti dal modello NER.


\section{Requisiti del sistema}

Alla luce delle criticità individuate, il sistema sviluppato deve soddisfarre una serie di requisiti comuni a entrambe le soluzioni analizzate, al fine di consentire un confronto significativo. La soluzione prodotta infatti non sarà direttamente integrata nell'applicativo esistente che è già operativo in produzione, ma fungerà da prototipo che dovrà in seguito ad una valutazione accurata essere poi rifinito e adeguato per poter costituire una nuova funzionalità \textit{production-ready}.

Per quanto riguarda i requisiti funzionali, possono essere schematizzati come di seguito.

\begin{enumerate}
    \item \textbf{Definizione delle entità rilevanti} : nonostante i documenti possano avere template diversi è necessario ricondurli tutti a una serie di entità comuni che saranno quelle da estrarre
    \item \textbf{Supporto all'addestramento di un modello NER} : Il sistema deve gestire il flusso di training di un modello NER personalizzato, quindi a partire dalla gestione dei dati di training, alla fase effettiva di addestramento e validazione, fino al salvataggio dell'output finale che verrà poi utilizzato per le funzioni di estrazione delle informazioni
    \item \textbf{Identificazione automatica delle entità} : il sistema dev'essere in grado di ricevere in input il PDF e estrarre le entità trovate secondo lo schema predefinito utilizzando un modello NER
    \item \textbf{Produzione di un output strutturato} : l'output dev'essere un insieme di dati strutturato secondo un formato ben preciso (es. JSON)
    \item \textbf{Normalizzazione dei dati} : Nei report buona parte dei dati pur rappresentando la stessa entità vengono spesso riportati in formati diversi, i dati risultanti dall'estrazione devono essere normalizzati il più possibile per facilitare la fase di ricerca dei risultati.
\end{enumerate}

Dal punto di vista non funzionale invece, il sistema deve rispecchiare alcuni aspetti tecnici legati al contesto applicativo in cui va inserito, ma anche ad alcune caratteristiche derivanti dagli obiettivi di progetto. Di seguito sono elencati i concetti principali di cui tenere presente durante lo sviluppo.

\textbf{Usabilità}. Pur non rappresentando il focus principale di questa fase prototipale, il sistema deve essere progettato in modo da poter essere integrato in un’interfaccia web semplice e intuitiva, consentendo una fruizione agevole anche da parte di utenti non tecnici coinvolti nel processo di caricamento e validazione dei report.

\textbf{Consistenza}. Il sistema deve garantire un comportamento coerente rispetto alla variabilità dei documenti in ingresso, riducendo il più possibile la dipendenza da specifici template o scelte di impaginazione adottate nei report PDF, così da assicurare risultati affidabili anche in presenza di documenti eterogenei.

\textbf{Modularità}. Considerata la natura sperimentale del progetto, è fondamentale adottare un’architettura modulare che consenta di sostituire, aggiornare o migliorare singole componenti del processo senza compromettere il funzionamento complessivo del sistema.

\textbf{Estendibilità}. Il sistema deve essere progettato in modo da supportare facilmente l’aggiunta di nuove entità informative da estrarre e l’adattamento a nuovi layout documentali, permettendo l’evoluzione della soluzione in funzione di esigenze future senza richiedere una riprogettazione completa.

\textbf{Isolamento}. Il sistema deve essere containerizzato per favorire l’integrazione in un’architettura a microservizi, garantendo l’isolamento delle singole componenti applicative e la comunicazione tramite interfacce ben definite.

\textbf{Riproducibilità}. Particolare attenzione deve essere posta alla riproducibilità del processo di training del modello NER, in modo da consentire iterazioni sperimentali controllate e permettere una valutazione oggettiva delle diverse soluzioni e configurazioni adottate nel corso del progetto.

Come già anticipato, considerata la natura sperimentale del progetto e la necessità di valutare soluzioni applicabili in un contesto reale, si è scelto di analizzare e confrontare due differenti approcci progettuali all’estrazione delle informazioni dai documenti differenti, nelle sezioni successive verranno esposte le due differenti architetture a livello logico.

\section{Approccio basato su soluzioni custom}

In primo luogo, è stata progettata una soluzione custom end-to-end che definisce sia una pipeline di addestramento di un modello di Named Entity Recognition sia un servizio applicativo che utilizza tale modello per la fase di estrazione automatica delle informazioni. L’obiettivo di questo approccio è valutare una soluzione maggiormente controllabile e adattabile al dominio applicativo considerato, consentendo di intervenire in modo puntuale sulle diverse fasi del processo.

La progettazione della soluzione custom ha portato alla definizione di due flussi logici distinti ma complementari: un flusso offline dedicato alla preparazione dei dati e all’addestramento del modello NER, e un flusso online destinato all’utilizzo del modello all’interno dell’applicazione per l’analisi dei documenti caricati dagli utenti. Tale separazione è stata introdotta intenzionalmente al fine di garantire modularità, favorire la sperimentazione e permettere la sostituzione o l’evoluzione di singole componenti senza impatti sull’intero sistema.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/flusso_custom_training.png}
    \caption{Flusso di training di un modello NER}
    \label{fig:flusso_custom_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/flusso_custom_service.png}
    \caption{Flusso di utilizzo del modello all’interno del servizio applicativo}
    \label{fig:flusso_custom_service}
\end{figure}

La Figura~\ref{fig:flusso_custom_training} mostra la pipeline di training del modello NER, il cui scopo è ottenere come output un modello addestrato a partire da documenti rappresentativi del dominio applicativo. Questo flusso comprende le fasi di estrazione del testo dai documenti PDF, preprocessing, segmentazione in finestre contestuali e annotazione manuale dei dati, culminando nella fase di addestramento del modello.

La Figura~\ref{fig:flusso_custom_service} illustra invece il flusso di utilizzo del modello all’interno del servizio applicativo. In questo caso, il modello NER addestrato viene impiegato per analizzare un documento fornito in input dall’utente, estrarre le entità informative di interesse e produrre un output strutturato, opportunamente normalizzato per facilitarne l’integrazione con i componenti applicativi a valle.

Nei paragrafi successivi vengono descritte nel dettaglio le singole fasi che compongono i due flussi, evidenziandone il ruolo all’interno dell’architettura complessiva e le motivazioni progettuali alla base delle scelte adottate.

\subsection{Estrazione del testo}

La prima fase della pipeline riguarda l’estrazione del contenuto testuale dai documenti PDF forniti in input al sistema (\textit{Text Extraction}). Tale operazione rappresenta un passaggio cruciale, in quanto i documenti oggetto di analisi non sono strutturati secondo formati pensati per l’elaborazione automatica, ma sono progettati principalmente per la consultazione umana.

I report di test analizzati presentano infatti caratteristiche eterogenee, quali layout complessi, utilizzo di tabelle, presenza di colonne multiple e contenuti multilingua. Per questo motivo, l’obiettivo della fase di estrazione non è quello di ottenere una rappresentazione perfettamente strutturata del documento, bensì di recuperare in modo affidabile il testo disponibile, preservando il più possibile le informazioni di posizione e di ordine necessarie alle fasi successive della pipeline.

L’output di questa fase consiste in una rappresentazione testuale del documento che include, per ciascun frammento estratto, dati strutturali come la pagina di appartenenza e la \textit{bbox}, ovvero le informazioni posizionali sul rettangolo che racchiude il testo, definito con una quadrupla di coordinate $(x_{min}, y_{min}, x_{max}, y_{max})$ che rappresentano la posizione in pixel a partire dall'angolo in alto a sinistra della pagina.

Questi dati costituiscono la base su cui vengono applicate le successive operazioni di preprocessing e segmentazione, in particolare per la ricerca di strutture tabellari all'interno del documento.

\subsection{Preprocessing}

A partire dall'estrazione testuale, è stata prevista una fase di preprocessing finalizzata a ridurre il rumore presente nei dati e a rendere il contenuto testuale più adatto all’elaborazione tramite modelli di Named Entity Recognition. I documenti analizzati, infatti, contengono spesso elementi non informativi o ridondanti, quali interruzioni di riga arbitrarie, spaziature irregolari, intestazioni ripetute e caratteri speciali introdotti dal processo di conversione del PDF.

Le operazioni di preprocessing hanno lo scopo di normalizzare il testo, mantenendo al contempo un compromesso tra pulizia dei dati e conservazione del contesto informativo. In particolare, vengono applicate trasformazioni volte a uniformare la formattazione del testo, ridurre le discontinuità non significative e semplificare la rappresentazione testuale senza alterare il contenuto semantico rilevante. Inoltre, in questa fase vengono utilizzate le informazioni posizionali per ricostruire le strutture tabellari.

Questa fase è stata progettata come un modulo indipendente della pipeline, in modo da poter essere facilmente adattata o estesa in funzione delle caratteristiche dei documenti analizzati o delle esigenze emerse durante le fasi di sperimentazione. L’output del preprocessing costituisce l’input per la successiva fase di segmentazione del testo.

\subsection{Segmentazione}

La fase di segmentazione (\textit{segmentation}) del testo ha l’obiettivo di suddividere il contenuto estratto e preprocessato in porzioni di dimensione controllata, dette finestre contestuali, che forniscono un input più significativo ai modelli NER. Tale scelta progettuale deriva dalla necessità di gestire documenti di lunghezza elevata, nei quali le informazioni rilevanti risultano distribuite su più pagine e in contesti testuali eterogenei.

Le finestre nel caso generale vengono costruite secondo criteri configurabili, tenendo conto dell’ordine del testo all’interno del documento e delle informazioni di posizione disponibili. Nel nostro caso si è scelto di includere nella finestra un numero fisso di righe, utilizzando una strategia di \textit{stride}, ovvero di una sovrapposizione controllata tra finestre consecutive.

I principali vantaggi di questo approccio sono i seguenti:

\begin{itemize}
    \item \textbf{Miglioramenti prestazionali e qualitativi della fase di training} : i modelli basati su \textit{transformer} infatti per la loro struttura beneficiano in entrambi gli aspetti indicati avendo delle porzioni ridotte di testo, rispetto a un unico lungo blocco testuale \cite{beltagy-longformer}.
    \item \textbf{Riduzione della \textit{boundary truncation}} : l'utilizzo di \textit{stride} limita il cosiddetto fenomeno della \textit{boundary truncation}, ovvero la possibilità che un'entità sia al confine tra due finestre, con il rischio che parte della entità stessa o del suo contesto sia nella finestra precedente / successiva.
    \item \textbf{Maggior contestualità a entità rare}: su documenti lunghi e multi-pagina, l'apprendimento per il riconoscimento di poche entità disperse all'interno del contenuto è difficoltoso. Tramite il \textit{windowing} e lo \textit{stride} è possibile avere più occorrenze delle varie entità in un contesto limitato, limitando l'ambiguità introdotta dal testo non correlato.
\end{itemize}



\subsection{Etichettatura}

La fase di etichettatura (\textit{labeling}) dei dati costituisce un passaggio fondamentale per l’addestramento supervisionato, in quanto definisce in modo esplicito la corrispondenza tra il testo e le entità informative di interesse. In questa fase, le finestre testuali generate durante la segmentazione vengono annotate manualmente secondo lo schema di etichette definito nella fase iniziale di analisi.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/labeling_example.png}
    \caption{Esempio di etichettatura}
\end{figure}

Al fine di garantire consistenza e qualità delle annotazioni, è necessario mantenere degli standard di uniformità, per esempio nella gestione degli spazi, nel mantenimento di un unico formato per entità sintatticamente uguali, evitando di definire entità non direttamente presenti nel testo ma revisionate manualmente.

L’adozione di tali criteri consente di ridurre l’ambiguità durante la fase di training e di fornire al modello esempi supervisionati coerenti, migliorandone la capacità di apprendere le caratteristiche distintive delle entità rilevanti nel dominio considerato. 

L'output ottenuto dalla fase di etichettatura consiste in un file contenente l'associazione tra le finestre contestuali e le relative etichette, indicate semplicemente da una tripla formata dagli indici posizionali di inizio e fine della stringa individuata all'interno del testo, e dalla stringa stessa.

\subsection{Addestramento}

La fase di addestramento (\textit{training}) del modello è stata implementata come un processo supervisionato a partire dai dati annotati manualmente tramite uno strumento di labeling. In particolare, le annotazioni vengono esportate e successivamente convertite in un formato nativo compatibile con la libreria scelta, necessario per l’avvio della fase di training.

Durante questa fase, il testo viene tokenizzato e le annotazioni vengono riallineate alla tokenizzazione interna del modello, adottando una strategia difensiva per gestire eventuali disallineamenti tra gli offset testuali e i confini dei token. Le entità che non risultano correttamente allineabili vengono scartate, evitando così errori durante l’addestramento e garantendo la stabilità del processo.

Al fine di consentire una valutazione oggettiva delle prestazioni del modello, il dataset annotato viene suddiviso automaticamente in un insieme di addestramento e un insieme di validazione, secondo una proporzione configurabile. Tale suddivisione viene effettuata in modo riproducibile, utilizzando un \textit{seed} casuale, così da rendere confrontabili le diverse iterazioni di training effettuate nel corso della sperimentazione.

[TODO spiegare addestramento]

\subsection{Utilizzo del modello NER}

[TODO]

\section{Approccio basato su servizi gestiti}

In alternativa alla soluzione custom progettata e descritta nelle sezioni precedenti, è stato considerato un approccio basato sull’utilizzo di servizi gestiti in cloud già esistenti sul mercato, in grado di fornire funzionalità avanzate di analisi documentale e di estrazione delle informazioni. Questo approccio si fonda sull’integrazione di un servizio esterno che espone API dedicate all’elaborazione di documenti non strutturati, delegando al provider cloud le fasi di analisi del contenuto e di riconoscimento delle entità informative.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/flusso_cloud_based.png}
    \caption{Flusso di integrazione di un servizio cloud-based}
    \label{fig:flusso_cloud_based}
\end{figure}

Dal punto di vista progettuale, l'implementazione interna è stata realizzata in maniera tale da semplificare il più possibile per ottenere rapidamente un prototipo funzionante, gestendo esternamente le fasi di feature engineering, di etichettatura e di addestramento. 

L’integrazione con un servizio cloud-based viene progettata come un componente indipendente all’interno dell’architettura applicativa, accessibile tramite interfacce REST e sostituibile senza impatti sulle altre componenti del sistema. In questo modo, il servizio di estrazione delle informazioni può essere visto come una "black box" che riceve in input un documento PDF e restituisce un output strutturato secondo uno schema predefinito, coerente con le esigenze dell’applicazione. Oltre a questo, il servizio si occuperà della fase finale di pulizia e normalizzazione dei dati restituiti dal servizio esterno, in modo da standardizzare l'output rendendolo coerente con quello definito tramite implementazione custom.

Questo approccio presenta tuttavia alcune implicazioni progettuali, legate principalmente alla dipendenza da un servizio esterno, alla limitata possibilità di personalizzazione del modello sottostante e alla questione della sicurezza dei dati nel cloud nell'ambito della gestione dei documenti di input. Lo scopo di tale implementazione è effettuare una valutazione analitica e prestazionale al fine di individuare la soluzione più valida da utilizzare in un ambiente di produzione.


\section{Architettura generale della soluzione}

[TODO]

\section{Strategie di associazione documento-entità}

[TODO]