\chapter{Implementazione}

Il presente capitolo descrive l’implementazione concreta delle soluzioni descritte fino a questo momento, evidenziando le tecnologie utilizzate e le modalità di integrazione tra i diversi moduli.

In particolare, vengono descritte le principali componenti implementative della soluzione custom basata su tecniche di Named Entity Recognition, nonché le modalità di utilizzo della soluzione cloud-based a fini di confronto. Il capitolo si concentra sugli elementi rilevanti per la realizzazione del prototipo, tralasciando i dettagli implementativi di basso livello, e pone l’attenzione sugli aspetti che incidono maggiormente sull'architettura progettuale.

\section{Tecnologie utilizzate}

\subsection{PyMuPDF}

Per l'estrazione del contenuto testuale dei documenti PDF nativi è stata utilizzata la libreria \textbf{PyMuPdf} \cite{pymupdf-docs} (anche nota come \textit{fitz}), una trasposizione in \textit{Python} del motore di parsing e rendering open source \textit{MuPDF}, progettato per l'elaborazione efficiente di documenti in formato PDF, XPS ed EPUB.

PyMuPDF consente di accedere direttamente alla rappresentazione interna del documento, permettendo di accedere alle sue componenti fondamentali, secondo il livello di granularità richiesto dall'applicativo. In particolare la libreria offre i seguenti livelli gerarchici di rappresentazione:

\begin{itemize}
    \item \textbf{Document}: restituisce la rappresentazione del file PDF nel suo complesso
    \item \textbf{Pages}: restituisce una rappresentazione in una sequenza ordinata di pagine
    \item \textbf{Blocks}: restituisce le porzioni logiche di contenuto (testo, immagini, disegni)
    \item \textbf{Lines / Spans}: restituisce unità testuali più finiti, contenenti stringhe di caratteri con attributi di stile
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/pymupdf_structure.png}
    \caption{Esempio della struttura gerarchica generata da PyMuPDF}
\end{figure}

Oltre alla rappresentazione testuale, PyMuPDF consente di accedere a informazioni aggiuntive relative al layout e metadati del documento, come coordinate spaziali dei blocchi testuali, font e dimensione dei caratteri. Queste informazioni possono essere utilizzate per migliorare l'interpretazione del contenuto, ad esempio distinguere intestazioni, tabelle o sezioni ricorrenti all'interno del testo.

La libreria si denota per elevata velocità di elaborazione grazie al motore su cui è basata, tuttavia non fornisce nativamente strumenti per l'interpretazione semantica delle strutture testuali estratte. Di conseguenza, PyMuPDF viene utilizzata come componente di basso livello all'interno di una pipeline più strutturata e costituisce l'input per successive fasi di analisi semantica e contestuale.

\subsection{spaCy}

Per l'implementazione del modello di Named Entity Recognition è stata utilizzata la libreria \textbf{spaCy}, una piattaforma open source per il \textit{Natural Language Processing} in Python \cite{spacy-docs}. spaCy fornisce un'architettura modulare e ottimizzata che consente di costruire pipeline di elaborazione del linguaggio naturale basate su modelli neurali addestrabili e personalizzabili.

spaCy adotta un'architettura a pipeline in cui il testo in ingresso viene progressivamente trasformato attraverso una sequenza di componenti. In primo luogo il testo in ingresso viene convertito in un oggetto \texttt{Doc}, che rappresenta la struttura dati centrale della libreria. Il \texttt{Doc} contiene una sequenza ordinata di token, ciascuno dei quali mantiene informazioni legate al testo originale, alla posizione e al contesto. Ogni componente riceve un oggetto \texttt{Doc} come input e può arricchirlo con annotazioni aggiuntive.

Il flusso di elaborazione può essere schematizzato come segue:

\begin{enumerate}
    \item tokenizzazione del testo
    \item costruzione delle rappresentazioni vettoriali dei token
    \item inferenza tramite il modello NER
    \item produzione delle entità rilevanti come segmenti contigui di token
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/spacy_pipeline.png}
    \caption{Pipeline di elaborazione di spaCy}
\end{figure}

La componente NER di spaCy è implementata come un modello di \textit{sequence labeling}, il cui obiettivo è assegnare a ciascun token un'etichetta che rappresenta il ruolo semantico del token stesso all'interno di un'entità. Il modello, nelle versioni più recenti della libreria, è costruito tramite un framework interno chiamato \textit{Thinc}, che permette di definire reti neurali modulari e configurabili tramite un apposito file di configurazione.

A partire da un modello di base, spaCy consente l'addestramento di modelli personalizzati mediante apprendimento supervisionato. Il processo di training richiede un dataset annotato manualmente, in cui le entità da riconoscere sono etichettate nel testo e tramite il file di configurazione è possibile definire l'architettura del modello, i parametri di ottimizzazione (\textit{learning rate}, \textit{batch size}, numero di iterazioni) e le modalità di valutazione. 

\subsection{Doccano}

Per la fase di etichettatura manuale dei dati è stato usato uno strumento open-source chiamato \textbf{Doccano}, una applicazione web-based progettata per l’annotazione di testi e altri task legati ai processi di machine learning \cite{doccano}. L'utilizzo di Doccano è motivato dalla necessità di disporre di un ambiente interattivo che consentisse di annotare in modo efficiente e controllato grandi quantità di testo.

La piattaforma supporta vari task legati ai dataset per i processi di machine learning, tra cui classificazione ed etichettatura di immagini o testi, in particolare per lo scopo del progetto fornisce una funzionalità chiamata \textit{Sequence Labeling}, in cui è possibile caricare dei file testuali in formati comuni (txt, JSON, \dots) e di esportare i dati annotati in formati compatibili con i tool di NLP, ottenendo già dei file da fornire come input per la fase di training del modello di NER.

\subsection{FastAPI}

Per l'implementazione dei servizi web del sistema è stato utilizzato il framework \textbf{FastAPI}, una libreria Python moderna per lo sviluppo di API RESTful, caratterizzata da elevate prestazioni, semplicità di utilizzo e supporto nativo per la validazione dei dati \cite{fastapi_docs}.

FastAPI è basato sullo standard \textit{ASGI (Asynchronous Server Gateway Interface)}, che permette di gestire richieste in parallelo grazie alle funzionalità asincrone offerte da Python. Inoltre la libreria contiene un'integrazione nativa con \textit{Pydantic}, la più comune libreria di Python per la validazione dei dati, che permette utilizzando dei modelli di definire in modo chiaro la struttura dei dati accettati e restituiti dagli endpoint, specificando tipi, campi obbligatori e vincoli di validazione.

L'adozione di FastAPI ha permesso di realizzare un layer di servizi web che espone alle altre componenti del sistema le funzionalità implementate in questo progetto, rispondendo al requisito di integrazione in un contesto applicativo definito da microservizi.

\subsection{Google Cloud Document AI}

Per l'implementazione del flusso NER utilizzando un servizio cloud-based già preconfigurato, è stato scelto di usare la suite di servizi di \textbf{Google Cloud}. Uno dei motivi che ha fatto pendere la scelta verso questa piattaforma piuttosto che le altre presenti sul mercato, è la presenza di un servizio dedicato all'analisi documentale, chiamato \textbf{Document AI}. Questo servizio è progettato per l'analisi automatica di documenti utilizzando tecniche di machine learning e fornisce una soluzione end-to-end per l'estrazione strutturata di informazioni rilevanti, racchiudendo l'intero ciclo di parsing del testo, riconoscimento del layout e analisi semantica del contenuto.

Il servizio di Document AI è strutturato tramite una divisione in microservizi denominati anche \textit{processori}, di cui ciascuno è specializzato in un compito nell'ambito della gestione documentale. La Figura~\ref{fig:docai_overview} riporta i principali casi d'uso con i relativi processori offerti.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/docai_overview.png}
    \caption{Diagramma delle funzionalità offerte da Document AI}
    \label{fig:docai_overview}
\end{figure}

I processori sono suddivisi per tre task principali:

\begin{itemize}
    \item \textbf{Digitalizzazione}. Comprende operazioni quali l'estrazione di testo e informazioni sul layout dei documenti utilizzando tecnologie OCR, con la possibilità di analizzare la qualità dell'immagine scansionata di un documento.
    \item \textbf{Estrazione}. Comprende operazioni quali l'estrazione di tabelle o di form tramite associazione chiave-valore e l'estrazione di entità rilevanti di contesti preaddestrati (ad esempio documenti medici, legali e contabili) o completamente personalizzate tramite etichettatura e addestramento.
    \item \textbf{Classificazione}. Comprende operazioni che prevedono la classificazione di documenti secondo criteri personalizzabili o anche la separazione di un documento multiclasse secondo classi personalizzate.
\end{itemize}

Ogni processore è legato a un singolo progetto, una volta creato quest'ultimo e configurato il processore scelto, viene fornita un'API REST che restituisce una rappresentazione strutturata JSON del documento passato in input demandando tutto il costo computazionale al servizio cloud e ai modelli sottostanti.

\section{Implementazione del modello NER}

\subsection{Struttura del progetto e organizzazione del codice}

L’implementazione è stata sviluppata e distribuita all’interno di un ambiente containerizzato mediante \textbf{Docker} \cite{docker}. Tale scelta progettuale consente di definire in modo esplicito e riproducibile l'ambiente di esecuzione dell'intero sistema, includendo le dipendenze software, le versioni delle librerie utilizzate e le configurazioni necessarie al corretto funzionamento delle diverse componenti. 

All’interno del container, il progetto è stato organizzato con l’obiettivo di separare in modo chiaro le responsabilità delle diverse componenti e favorire la manutenibilità e l’estendibilità del sistema. In particolare questa è la suddivisione delle cartelle di progetto:

\begin{itemize}
    \item \textbf{app} : contiene il servizio web esposto dal container, in particolare il file \texttt{main.py} contiene l'inizializzazione del server, mentre il resto delle logiche è implementato nelle sottocartelle indicate di seguito.
    \begin{itemize}
        \item \textbf{api} : contiene le routes esposte
        \item \textbf{schemas} : contiene i modelli di dati strutturati utilizzati nell'applicazione
        \item \textbf{services} : contiene le logiche applicative suddivise per funzione
        \item \textbf{utils} : contiene funzioni di utilità comuni ad esempio per la fase di normalizzazione dei dati
    \end{itemize}
    \item \textbf{data} : rappresenta lo storage dell'applicativo, in particolare contiene il dataset per il training, sia nel loro formato iniziale ovvero PDF, sia i relativi file ottenuti dalle varie fasi di estrazione del testo. Inoltre contiene il modello addestrato che viene utilizzato per le funzioni di NER ed eventuali configurazioni.
    \item \textbf{scripts} : contiene le componenti applicative della pipeline di training
\end{itemize}

Questa organizzazione consente di mantenere una netta separazione tra le fasi sperimentali di sviluppo del modello e le componenti destinate all’utilizzo operativo, riducendo l’accoppiamento tra le parti e rendendo il sistema più semplice da estendere o modificare nel tempo.

\subsection{Estrazione del testo e preparazione del dataset per l'etichettatura}

Per la preparazione del dataset alla fase di etichettatura è stato realizzato un batch che esegue in sequenza tre script Python, altamente configurabili attraverso tuning dei parametri. Il batch prende in input una cartella da cui leggere i documenti PDF e restituisce un file \textit{JSONL}, formato accettato da Doccano per la fase di etichettatura.

Il primo script \texttt{export\_lines\_json} si occupa di trasformare il PDF nativo in una rappresentazione strutturata e utilizzabile per la fase di segmentazione, a tale scopo viene utilizzato PyMuPDF, che grazie alle informazioni spaziali e di impaginazione consente di applicare funzioni di elaborazione che rendono il testo più pulito e organizzato rispetto a una semplice estrazione grezza. Lo script lavora pagina per pagina a partire dal testo grezzo estratto, su cui esegue le seguenti operazioni:

\begin{enumerate}
    \item \textbf{Pulizia del testo}. Viene effettuata una rimozione di spazi, caratteri speciali o non utili ai fini del training (es. in alcuni documenti è presente sotto alla dicitura in inglese la rispettiva traduzione in cinese).
    \item \textbf{Riconoscimento delle strutture tabellari}. PyMuPDF offre una funzione \texttt{Page.get\_drawings()} che permette di estrarre tutte le grafiche vettoriali presenti all'interno di una pagina e ne riconosce la tipologia. Questa funzione viene utilizzata per estrarre le righe verticali e i rettangoli che formano una tabella, al fine di ricostruire l'ordine logico del testo e usare dei separatori (\texttt{|}) per simulare la struttura tabellare. Nel caso non siano disponibili o non siano affidabili le informazioni sui disegni vettoriali, si cerca di fare clustering sulla coordinata \texttt{x} per individuare delle colonne logiche.
    \item \textbf{Suddivisione in righe e Clustering}. Facendo clustering sulla coordinata $y$ invece le parole vengono raggruppate in linee visive, il risultato è un insieme righe ordinato in base alla posizione sulla pagina.
    \item \textbf{Merge delle righe collegate e pulizia finale}. A partire dalle righe e dalla loro rappresentazione in colonne (a fianco del testo grezzo con i separatori si ottiene anche un \textit{array} di stringhe) viene applicata una fase di merging che va ad esempio a individuare righe dove una sola colonna è valorizzata e le altre no, in questo caso siamo di fronte a una continuazione della colonna soprastante. Infine si applica una pulizia finale che va a rimuovere righe riconosciute come rumorose ad esempio vari header/footer.
\end{enumerate}

Di seguito un esempio di riga ottenuta come output dalle operazioni appena elencate:


\begin{lstlisting}
{
    "y": 300.2481384277344,
    "bbox": [
        37.919986724853516, 300.2481384277344, 
        548.5767211914062, 322.925537109375
    ],
    "cols": [
        "", "Colorfastness to Alkaline perspiration", 
        "Grade", "GB/T 3922-2013", "Color staining > 3, 
        "Polyester 4-5", "Pass"
    ],
    "text": "Colorfastness to Alkaline perspiration | Grade | GB/T 3922-2013 | Color staining > 3 | Polyester 4-5 | Pass"
},
\end{lstlisting}

La fase successiva, ovvero la segmentazione in finestre testuali, viene gestita dallo script \texttt{build\_spacy\_windows.py}. La necessità di creare finestre nasce dalla necessità in primo luogo di non dover fornire al modello l'intero testo estratto dal PDF, per evitare un input rumoroso e efficientarne le prestazioni; da un altro punto di vista, le singole righe invece spesso non contengono sufficiente contesto locale per interpretare correttamente le entità.

Dal punto di vista implementativo, lo script ordina le linee per pagina e posizione verticale e le unisce in finestre testuali usando principalmente due parametri: la dimensione della finestra (\texttt{window\_size}) e il passo di avanzamento (\texttt{stride}). Il tuning di questi due parametri permettono di bilanciare copertura del documento e ridondanza: \texttt{stride} ridotto aumenta la probabilità che che un'entità compaia integralmente almeno in una finestra, mentre valori più elevati riducono il numero totale di esempi da annotare. Per quanto riguarda la \texttt{window\_size} i migliori risultati sia a livello di efficienza che di precisione del modello sono stati riscontrati con finestre da 7 righe. Una volta create le finestre vengono applicate ulteriori funzioni di pulizia usando \textit{regex} per individuare finestre con testo molto rumoroso o ritenuto non rilevante, in modo da evitare di avere troppe finestre senza entità rilevanti al loro interno.

L'ultima fase della preparazione del dataset è gestita dallo script \\\texttt{windows\_to\_spacy\_jsonl.py}, che rielabora gli output precedenti per preparare il file di input della fase di etichettatura rispettando un formato supportato da Doccano.
Di seguito è possibile vedere un esempio di un frammento del file relativo a una finestra testuale:

\begin{lstlisting}
    {
        "text": "BLOCCO TESTUALE DELLA FINESTRA", 
        "label": [], 
        "meta": {
            "source_pdf": "/data/pdf/15-097-66240980285.pdf", 
            "page": 1, 
            "window": 7, 
            "stride": 2, 
            "start_line": 2, 
            "end_line": 8
        }
    }
\end{lstlisting}

il campo \texttt{label} verrà riempito dalla fase di etichettatura contenendo le entità trovate all'interno della finestra testuale, mentre i metadati dentro la chiave \texttt{meta} non vengono direttamente utilizzati dal modello ma servono più per debug e tracciabilità.

\subsection{Feature engineering ed etichettatura}

In questa fase è stato utilizzato il tool open-source Doccano, avviabile in un webserver locale anche tramite Docker, con i seguenti comandi:

\begin{lstlisting}[language=bash]
docker pull doccano/doccano
docker container create --name doccano \
  -e "ADMIN_USERNAME=admin" \
  -e "ADMIN_EMAIL=admin@example.com" \
  -e "ADMIN_PASSWORD=password" \
  -v doccano-db:/data \
  -p 8000:8000 doccano/doccano
\end{lstlisting}

La creazione di un utente \textit{admin} è necessaria perché la piattaforma può essere gestita a tutti gli effetti con un sistema di utenti con ruoli e permessi specifici. Una volta aperta l'interfaccia e creato il progetto, per poter iniziare l'etichettatura è necessario innanzitutto creare le etichette tramite l'apposita pagina \textit{Labels}, dove la fase di selezione delle entità rilevanti è stata effettuata seguendo i criteri indicati nella Sezione~\ref{sec:associazione-documento-entita}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/labels_doccano.png}
    \caption{Pagina di creazione etichette di Doccano}
\end{figure}

A questo punto dalla pagina \textit{Dataset} è possibile importare il file ottenuto negli step precedenti, e per ogni riga presente nel file verrà creato un frammento di testo da annotare, a cui è assegnato un identificativo univoco e su cui è possibile cliccare per procedere con l'annotazione delle entità. Una volta evidenziate nel testo le porzioni rilevanti e assegnate alle etichette, è necessario contrassegnare il frammento come \textit{checked} per indicare che l'operazione è completata, e sulla parte destra della pagina è possibile verificare lo stato corrente delle annotazioni effettuate.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/annotations_doccano.png}
    \caption{Etichettatura delle finestre testuali su Doccano}
\end{figure}

Una volta terminato il processo manuale di etichettatura, è possibile esportare i dati sempre in formato JSONL con il campo \texttt{label} riempito per ogni finestra testuale da una lista di triple formate da:

\begin{itemize}
    \item indice del carattere iniziale del valore etichettato
    \item indice del carattere finale del valore etichettato
    \item stringa con il nome dell'etichetta assegnata
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/export_doccano.png}
    \caption{Pagina di esportazione dati di Doccano}
\end{figure}

\subsection{Addestramento del modello}

\subsubsection{Costruzione del dataset e addestramento del modello}

Il file JSONL, pur essendo semplice e leggibile, non è direttamente utilizzabile dal motore di training utilizzato ovvero spaCy. Per questo motivo è stato introdotto un ulteriore step realizzato tramite script Python \texttt{jsonl\_to\_docbin.py}, con lo scopo di convertire il dataset annotato nel formato binario nativo di spaCy (\texttt{DocBin}), richiesto dalla procedura di addestramento.

Nel dettaglio il seguente blocco di codice estratto dallo script fornisce la logica con cui viene inizializzato il modello.

\begin{lstlisting}[language=Python]
random.seed(args.seed)

nlp = spacy.blank(args.lang)

examples = []
with open(args.inp, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)

        text = obj.get("text", "")
        labels = obj.get("label", [])  # Doccano field

        doc = nlp.make_doc(text)
        ents = []
        for start, end, lab in labels:
            start = int(start); end = int(end)
            span = doc.char_span(start, end, label=lab, alignment_mode="contract")
            if span is None:
                continue
            ents.append(span)

        doc.ents = ents
        examples.append(doc)

if not examples:
    raise SystemExit("No examples found. Input file is empty")

random.shuffle(examples)
split = int(len(examples) * (1.0 - args.dev_ratio))
train_docs = examples[:split]
dev_docs = examples[split:]

train_bin = DocBin(store_user_data=True)
dev_bin = DocBin(store_user_data=True)

for d in train_docs:
    train_bin.add(d)
for d in dev_docs:
    dev_bin.add(d)
\end{lstlisting}

In primo luogo l'inizializzazione del modello avviene con la funzione \\ \texttt{spacy.blank()}, a cui viene fornita in input solo la lingua passata come argomento che indirizza in modo corretto la fase di tokenizzazione.

Una volta caricato il file JSONL ottenuto dalla fase di etichettatura, per ogni riga del file (che corrisponde a una finestra testuale), viene creato un oggetto \texttt{Doc} ovvero il testo tokenizzato, a cui poi vengono collegate le etichette tramite la funzione \texttt{Doc.char\_span()} che va a definire uno \texttt{Span}, ovvero una porzione di token.

A questo punto vengono definiti i dataset di addestramento e valutazione in formato \texttt{DocBin}, ideale per ottimizzare le prestazioni del motore di spaCy, secondo la proporzione definita dall'argomento \texttt{dev\_ratio}, ovvero il valore in percentuale del dataset di evaluation. Per fare ciò prima della divisione dei dataset vengono mischiati tutti i campioni tramite un seed passabile come argomento al fine di rendere il processo di training osservabile e ripetibile.

Al termine dello script i due dataset vengono salvati pronti per essere utilizzati nella fase di training, la quale viene eseguita direttamente da terminale all'interno del container utilizzando il seguente comando:

\begin{lstlisting}
docker compose run --rm ocr-ml \
python -m spacy train /data/spacy/config.cfg \
--output /data/spacy/model \
--paths.train /data/spacy/train.spacy \
--paths.dev /data/spacy/dev.spacy
\end{lstlisting}

Il processo genera nella cartella indicata di output una directory contenente i vari modelli ottenuti nel ciclo di addestramento, con una \texttt{model\_best} contenente la configurazione con i migliori risultati ottenuti sui dati di valutazione.

\subsection{Servizio web per l'estrazione delle entità}

Per consentire l'utilizzo del modello NER in un contesto applicativo, è stato realizzato un servizio web in Python basato su FastAPI. Il servizio espone un endpoint REST che riceve in input un documento PDF e restituisce un output strutturato contenente le entità riconosciute. Il flusso operativo del servizio può essere sintetizzato nei seguenti passaggi:
\begin{enumerate}
    \item \textbf{Acquisizione del documento}: La classe \texttt{UploadFile} consente di gestire il file come \textit{multipart/form-data} e ricevarne il contenuto.
    \item \textbf{Parsing e segmentazione}: il PDF viene elaborato per estrarre e segmentare il contenuto testuale in finestre, applicando la stessa logica utilizzata nella pipeline di training, al fine di garantire coerenza tra fase di addestramento e fase di inferenza.
    \item \textbf{Predizione}: ciascuna finestra viene processata dal modello spaCy addestrato, ottenendo le \textit{label} estratte.
    \item \textbf{Post-processing e normalizzazione}: le entità estratte vengono aggregate e normalizzate in un formato JSON coerente con le esigenze applicative, indipendente dal motore di estrazione (modello custom o soluzione cloud-based).
\end{enumerate}

\begin{lstlisting}[language=Python]
@router.post("/parse", response_model=SpacyParseResponse)
async def spacy_parse(
	file: UploadFile = File(...),
	pdf_parser: PdfParserService = Depends(get_pdf_parser_service),
	ner: SpacyNerService = Depends(get_spacy_ner_service),
):
	if file.content_type != "application/pdf":
		raise HTTPException(status_code=415, detail=f"Unsupported content_type: {file.content_type}")

	content = await file.read()
	if not content:
		raise HTTPException(status_code=400, detail="Empty file")

	parsed = pdf_parser.parse(content)
	windows = parsed["windows"]

	# windows -> NER
	preds = ner.predict_windows(windows, normalize=True)

	normalized = normalize_spacy_entities(by_window=preds, debug=False)

	return {
		"filename": file.filename,
		**normalized
	}
\end{lstlisting}

Un aspetto rilevante per l'uso operativo riguarda la gestione del caricamento del modello spaCy. Poiché l'inizializzazione del modello può essere onerosa, il modello viene caricato una sola volta e riutilizzato per le richieste successive, evitando costi ripetuti e migliorando la latenza. Questo poiché FastAPI fornisce la classe \texttt{Depends} che consente di fare \textit{dependency injection} integrando delle classi di servizi, in particolare il caricamento del modello avviene all'inizializzazione del servizio \texttt{SpacyNerService}, mentre \texttt{PdfParserService}, si occupa della fase preliminare di estrazione del testo e segmentazione.

\section{Fine-tuning con Google Cloud}

\subsection{Creazione del Processore}

\textbf{Configurazione iniziale}

La prima fase della realizzazione della pipeline che sfrutta le tecnologie di Google Cloud Document AI consiste nella creazione del progetto cloud e nella configurazione del relativo \textit{Service Account}. Il Service Account rappresenta l’identità applicativa utilizzata per accedere ai servizi Google tramite credenziali dedicate, le quali consentono la generazione di \textit{access token} successivamente utilizzati dall’applicazione per invocare le API del servizio.

Uno degli aspetti fondamentali del Service Account è la possibilità di definire in modo granulare i permessi associati, limitando l’accesso esclusivamente alle funzionalità necessarie al progetto. La gestione delle autorizzazioni può avvenire secondo due livelli principali di granularità:

\begin{itemize}
    \item \textbf{Ruoli}. Permettono di includere le autorizzazioni necessarie al funzionamento di uno o più specifici servizi di Google, ad esempio per l'utilizzo delle API di Document AI è necessario inserire il ruolo \\ \texttt{roles/documentai.apiUser}. È possibile anche definire dei ruoli personalizzati racchiudendo ruoli e permessi di diverse funzionalità.
    \item \textbf{Permessi}. Permettono di definire in modo esatto le singole azioni ammesse anche all'interno di un singolo servizio, ad esempio un utente che dovrà avere accesso al bucket dei dati in sola lettura avrà solo il permesso \texttt{storage.bucketOperations.get}, cosa non possibile utilizzando i ruoli.
\end{itemize}

Completata la configurazione del Service Account ed esportate le credenziali sotto forma di file JSON, si procede alla creazione del processore, che rappresenta il componente centrale del servizio di estrazione documentale utilizzato nel progetto.

Tramite l'interfaccia di Document AI è necessario in primo luogo selezionare la tipologia di processore, che come descritto in precedenza si diversificano oltre che per la funzione svolta anche in soluzioni già preaddestrate, come il modulo OCR che si limita all'estrazione del testo da un documento, e soluzioni custom, che pur partendo sempre da un modello preaddestrato consentono una gestione personalizzata della fase di \textit{fine-tuning} al fine da adattare il modello al contesto d'uso. Per gli scopi di questo lavoro è stato utilizzato il \textbf{Custom Extractor}, per adattare l'estrattore fornito da Google al contesto dei report di laboratorio. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/elenco_processors.png}
    \caption{Pagina di selezione dei processori di Document AI}
\end{figure}

\textbf{Feature engineering}

Una volta scelto il tipo di processore, la prima fase è quella della definizione delle entità rilevanti. Le entità vengono create con una comoda interfaccia che permette di definire i seguenti attributi:

\begin{itemize}
    \item \textit{Name} : identificativo unico per l'etichetta
    \item \textit{Parent} : è possibile selezionare un'altra etichetta che fungerà da "padre". A differenza di quanto visto nell'implementazione con spaCy, Document AI infatti consente di definire delle etichette composte. Ad esempio: invece di utilizzare due etichette \texttt{TEST\_NAME} e \texttt{TEST\_METHOD}, in questo caso possiamo definire un'etichetta \texttt{test} con due etichette figlie \texttt{name} e \texttt{method}.
    \item \textit{Method} : un'altra peculiarità di Document AI è la possibilità di definire delle etichette non estratte direttamente dal testo ma derivate tramite analisi del contesto. In questo caso d'uso è stato utile definire tre etichette derivate dal contesto:
    \begin{itemize}
        \item \texttt{language}, corrispondente alla lingua del documento, utile per effettuare traduzioni a posteriori e avere i dati coerenti a prescindere dal template utilizzato;
        \item \texttt{template}, una stringa identificativa dello specifico template del report, anche questa utile per valutazioni a posteriori;
        \item \texttt{item\_type}, una stringa che può assumere i valori \textit{article} o \textit{style}, indica se il modello sottoposto ai test è un articolo o un modello finito.
    \end{itemize}
    \item \textit{Data Type} : è possibile specificare la tipologia del dato, per ottenere delle normalizzazioni del dato già in fase di estrazione. Ad esempio è possibile specificare un campo come \textit{datetime}, e nell'estrarre il dato verrà già restituito in formato \texttt{YYYY\-MM\-DD}.
    \item \textit{Occurrence} : un'indicazione sul numero di occorrenze che il modello può estrarre per l'etichetta in questione da un singolo documento. È possibile specificare se un'etichetta può comparire una o più volte all'interno di un documento, o anche se può essere assente all'interno del singolo documento.
    \item \textit{Description} : è un campo testuale libero, in cui è possibile fornire maggiori dettagli sull'etichetta da estrarre. Permette di fare \textit{prompt engineering} sul modello, garantendo un maggiore livello di personalizzazione e aderenza al contesto.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/features_docai.png}
    \caption{Pagina di creazione delle etichette}
\end{figure}

\textbf{Gestione del dataset ed etichettatura}

Il passo successivo è la preparazione del dataset per il fine-tuning del modello, che prevede innanzitutto il collegamento col servizio di storage da cui è possibile attingere selezionando le cartelle da cui estrarre i dati. In questa fase è possibile anche definire la suddivisione dei documenti tra \textit{training} e \textit{evaluation}, definendolo esplicitamente o lasciando la suddivisione al sistema che suddividerà l'80\% dei documenti per l'addestramento e il restante 20\% per la valutazione.

Una volta caricati i documenti, è possibile tramite l'interfaccia visualizzarli uno per volta per effettuare l'etichettatura, la quale consiste nel selezionare l'area del documento rilevante e assegnarla a una delle etichette create in precedenza.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/etichettatura_docai.png}
    \caption{Esempio di etichettatura di un documento}
\end{figure}

\textbf{Fine-tuning del modello}

Una volta preparato il dataset, si procede andando ad effettuare il fine-tuning di un modello preaddestrato di Google, al fine di migliorarne l'aderenza al contesto specifico. L'interfaccia permette di creare una versione, ovvero un'istanza del modello addestrato, definendo solo come parametri il numero di passi di addestramento e un moltiplicatore al \textit{learning rate} predefinito del modello.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fine_tuning_docai.png}
    \caption{Interfaccia di creazione del modello personalizzato}
\end{figure}

Al termine dell'elaborazione il modello risultante viene deployato ed è accessibile sia per l'analisi dei risultati, forniti sia a livello globale che per singola etichetta. Per l'utilizzo del modello tramite API Google fornisce due identificativi ovvero il \textit{processor\_id} e il \textit{version\_id}, con i quali è possibile individuare l'istanza a cui affidare l'elaborazione dei file passati come parametro.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{figures/risultati_docai.png}
    \caption{Interfaccia di visualizzazione dei risultati dell'addestramento}
\end{figure}

\subsection{Integrazione di Google Document AI in un servizio web}

In parallelo alla soluzione custom basata su modello NER addestrato internamente, è stato implementato un servizio web alternativo che fa orchestratore tra l'interfaccia web e l'API esposta dal provider esterno.

L'endpoint REST, realizzato anch'esso mediante FastAPI, ha la medesima struttura a livello di input e output. La differenza sta nel fatto che il file viene inoltrato al servizio Google Document AI utilizzando l’SDK ufficiale Python, previa autenticazione tramite Service Account. L’invocazione del servizio avviene specificando il progetto, la regione e l’identificativo del processor precedentemente configurato su Google Cloud.

\begin{lstlisting}[language=Python]
@router.post("/parse", response_model=DocAiParseResponse)
async def docai_parse(
	file: UploadFile = File(...),
	docai: DocumentAiService = Depends(get_docai_service),
):
	if file.content_type not in ("application/pdf", "image/tiff", "image/png", "image/jpeg"):
		raise HTTPException(status_code=415, detail=f"Unsupported content_type: {file.content_type}")

	content = await file.read()
	if not content:
		raise HTTPException(status_code=400, detail="Empty file")

	try:
		result = docai.process_bytes(content, file.content_type)
	except Exception as e:
		raise HTTPException(status_code=502, detail=f"Document AI error: {e}")

	doc = result.document
	parsed = normalize_docai_entities(doc, debug=DEBUG)

	return {
		"filename": file.filename,
		**parsed,
	}   
\end{lstlisting}

Come si può notare dal codice in quest'implementazione la dipendenza è un \texttt{DocumentAiService} che si occupa dell'inizializzazione del client Google Cloud e dell'invocazione dei servizi esterni. La funzione \texttt{process\_bytes} contiene la seguente logica:

\begin{enumerate}
    \item Creazione di un oggetto \texttt{RawDocument} passando direttamente il contenuto in bytes del PDF, per convertirlo in un oggetto accettato da Document AI.
    \item Creazione del payload della richiesta, incapsulato in un oggetto \\ \texttt{ProcessRequest} che prende in input il \texttt{RawDocument} e il nome del processore
    \item Chiamata verso il servizio di inferenza di Document AI e restituzione dell'output fornito.
\end{enumerate}

\begin{lstlisting}[language=Python]
def process_bytes(self, content: bytes, mime_type: str):
    raw_document = documentai.RawDocument(content=content, mime_type=mime_type)
    request = documentai.ProcessRequest(name=self.processor_name(), raw_document=raw_document)
    return self.client.process_document(request=request)
\end{lstlisting}

In questo caso la fase di normalizzazione prevede una rielaborazione dell'output fornito da Google che è molto strutturato e ricco di metadati. Tra le varie informazioni che vengono fornite su cui è possibile fare delle valutazioni a posteriori, per ogni entità predetta viene indicato un valore \texttt{confidence}, compreso tra $0$ e $1$, che indica la probabilità di correttezza del dato estratto. Nella fase di normalizzazione è stato scelto di escludere tutte le entità con una \texttt{confidence} inferiore a $0.50$, per evitare nell'uso applicativo di utilizzare filtri sui dati tendenzialmente non corretti.

\section{Integrazione con l’applicazione esistente}

L'integrazione dei servizi sviluppati con FastAPI all'interno dell'applicativo esistente è fatta sfruttando la rete \textit{bridge} condivisa di Docker, definita nel \textit{docker-compose}, ovvero il file di configurazione che regola tutti i container e le loro configurazioni. Il container dedicato alle funzionalità NER è stato aggiunto alla rete comprendente le altre componenti dell'applicazione tra cui il frontend, implementato come una webapp in Angular.

Dal punto di vista dell'interfaccia l'obiettivo è affiancare le pagine già esistenti, che consentono tramite form specifici per articoli e prodotti finiti di filtrare i dati delle prove di laboratorio e una volta ottenuti i dati desiderati associargli le informazioni del documento di report PDF.

La nuova interfaccia realizzata è più semplice e immediata, e a livello operativo si occupa di:

\begin{itemize}
    \item caricare un report PDF;
    \item tramite le entità rilevate dal modello NER avviare la ricerca dei record sul database;
    \item mostrare le corrispondenze trovate e permettere all'utente la verifica e la selezione dei test prima della conferma.
\end{itemize}

[inserire immagini]

